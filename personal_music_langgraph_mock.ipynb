{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c158de7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60407c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "assert os.getenv(\"OPENAI_API_KEY\"), \"OPENAI_API_KEY 가 .env에 없어요. (.env에 OPENAI_API_KEY=... 추가)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bfe84b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Literal, TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "Valence = Annotated[float, Field(ge=-1.0, le=1.0)]      # 정서의 밝기(부정 - 긍정)\n",
    "Arousal = Annotated[float, Field(ge=0.0, le=1.0)]       # 각성도(에너지)\n",
    "Confidence = Annotated[float, Field(ge=0.0, le=1.0)]    # 확신도\n",
    "BPM     = Annotated[int,   Field(ge=50,  le=140)]       # 분당 비트수(BPM)\n",
    "DurSec  = Annotated[int,   Field(ge=5,   le=60)]        # 음악 길이(5초 - 60초) ※임시\n",
    "\n",
    "class EmotionResult(BaseModel):     # 감정 분석 결과의 틀\n",
    "    primary: str                    # 행복/슬픔/불안 같은 주요 감정 라벨 (문자열)\n",
    "    valence: Valence = 0.0          # 밝기(-1~1) 기본값 0\n",
    "    arousal: Arousal = 0.5          # 각성도(0~1) 기본값 0.5\n",
    "    confidence: Confidence = 0.7    # 확신도(0~1) 기본값 0.7\n",
    "    reasons: str = \"—\"              # 감정 판단의 근거 요약 기본은 —\n",
    "\n",
    "class MusicBrief(BaseModel):        # 음악 생성 전에 필요한 설계서(브리프) 스키마\n",
    "    mood: str                       # calm/uplifting/melancholic 같은 전반 무드\n",
    "    bpm: BPM = 90                   # 50~140 범위의 정수 기본 90\n",
    "    key: str = \"C major\"            # 조성. 예: “C major”, “A minor”\n",
    "    duration_sec: DurSec = 20       # 5~60 범위의 정수(초). 기본 20\n",
    "    instruments: List[str] = []     # 핵심 악기들. 예: [\"piano\", \"strings\"].\n",
    "    style_tags: List[str] = []      # 스타일 태그. 예: [\"ambient\", \"cinematic\"].\n",
    "    prompt: str                     # 텍스트-투-뮤직 모델에 줄 영문 프롬프트 (필수)\n",
    "\n",
    "class GraphState(TypedDict, total=False):\n",
    "    user_text: str                              # 사용자 입력 문장\n",
    "    use_external: bool                          # 외부 모델(예: Replicate) 쓸지 여부\n",
    "    emotion: EmotionResult                      # 위 스키마로 검증된 감정 결과\n",
    "    brief: MusicBrief                           # 위 스키마로 검증된 음악 브리프\n",
    "    audio_path: str                             # 생성된 오디오 파일 경로\n",
    "    provider_used: Literal[\"replicate\", \"mock\"] # 어떤 방식으로 생성했나\n",
    "    meta: Dict                                  # 그 외 로그/디버그용 메타데이터\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222c3e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm():\n",
    "    # OpenAI Chat 모델을 래핑한 LangChain 객체\n",
    "    # temperature=0.2 → 더 일관된(덜 랜덤) 출력\n",
    "    return ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
    "\n",
    "#스키마 강제(Structured Output), 감정 라벨링: 0.0 ~ 0.3\n",
    "#음악 브리프 같이 살짝 창의성 필요: 0.3 ~ 0.6\n",
    "#가사/스토리처럼 창작성 극대화: 0.7 ~ 1.0 (형식 깨질 수 있음)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3f9579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_emotion_node(state: GraphState) -> GraphState: # 감정 분석 노드\n",
    "    llm = get_llm()\n",
    "\n",
    "    # 시스템 지시문: 역할/출력 형식을 LLM에게 명확히 지정\n",
    "    sys = (\n",
    "        \"당신은 심리 정서를 요약하는 분석가입니다. \"\n",
    "        \"사용자 텍스트로부터 주요 감정을 한 단어(또는 짧은 구)로 도출하고, \"\n",
    "        \"valence(-1~1)와 arousal(0~1), confidence(0~1)를 추정하세요. \"\n",
    "        \"반드시 주어진 JSON 스키마(EmotionResult)에 맞춰 응답하세요.\"\n",
    "    )\n",
    "\n",
    "    # 사용자 입력은 state에서 꺼냄 (그래프의 '가방'에서 user_text를 읽어오는 느낌)\n",
    "    user_msg = state['user_text']\n",
    "\n",
    "    # LLM 출력이 EmotionResult 스키마에 '자동으로 검증/파싱'되도록 래핑\n",
    "    structured = llm.with_structured_output(EmotionResult)\n",
    "\n",
    "    # 메시지 형식: system + user\n",
    "    result = structured.invoke([\n",
    "        {\"role\": \"system\", \"content\": sys},\n",
    "        {\"role\": \"user\",   \"content\": user_msg}\n",
    "    ])\n",
    "\n",
    "    # 결과를 state에 저장하고 다음 노드가 쓰게 한다\n",
    "    state['emotion'] = result\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80e66319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose_brief_node(state: GraphState) -> GraphState:\n",
    "    llm = get_llm()                            # 1) 같은 LLM 설정 재사용\n",
    "    emo: EmotionResult = state[\"emotion\"]      # 2) 방금 저장한 감정 결과 꺼내기\n",
    "\n",
    "    # 3) 시스템 지시문: 출력 형식(브리프 규칙)과 제약(bpm, duration, key)을 설명\n",
    "    sys = (\n",
    "        \"너는 음악 감독이다. 아래 감정 분석 결과와 사용자 텍스트를 참고해, \"\n",
    "        \"치유/안정 목적의 짧은 BGM에 적합한 Music Brief를 JSON으로 만들어라. \"\n",
    "        \"bpm은 50~140, duration_sec는 5~60 사이여야 하며, key는 'C major' 같은 형식. \"\n",
    "        \"prompt는 영어로, 핵심 악기/무드/다이내믹을 간결히 포함할 것.\"\n",
    "    )\n",
    "\n",
    "    # 4) 사용자 메시지: 감정 수치와 원문 텍스트를 함께 전달\n",
    "    usr = (\n",
    "        f\"# Emotion\\n\"\n",
    "        f\"primary={emo.primary}, valence={emo.valence}, arousal={emo.arousal}, confidence={emo.confidence}\\n\\n\"\n",
    "        f\"# Text\\n{state['user_text']}\\n\"\n",
    "    )\n",
    "\n",
    "    # 5) MusicBrief 스키마로 구조화 출력 강제\n",
    "    structured = llm.with_structured_output(MusicBrief)\n",
    "    brief = structured.invoke([\n",
    "        {\"role\": \"system\", \"content\": sys},\n",
    "        {\"role\": \"user\",   \"content\": usr}\n",
    "    ])\n",
    "\n",
    "    state[\"brief\"] = brief                    # 6) 다음 노드에서 쓰도록 가방에 담기\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bc2e88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 7: 음악 생성 노드 (Replicate or Mock) ---\n",
    "import time\n",
    "import wave\n",
    "import struct\n",
    "import requests\n",
    "import numpy as np\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# 7-1) 간단 WAV 저장기 (16-bit PCM, 모노)\n",
    "def write_wav(path: str, sr: int, audio: np.ndarray):\n",
    "    data = np.clip(audio, -1.0, 1.0)\n",
    "    data = (data * 32767.0).astype(np.int16)  # float[-1,1] -> int16\n",
    "    with wave.open(path, 'wb') as wf:\n",
    "        wf.setnchannels(1)      # mono\n",
    "        wf.setsampwidth(2)      # 16-bit\n",
    "        wf.setframerate(sr)\n",
    "        wf.writeframes(data.tobytes())\n",
    "\n",
    "# 7-2) Mock 합성기: 간단한 사인파 멜로디(스케일 + BPM 기반)\n",
    "NOTE_FREQ = {\n",
    "    \"C4\": 261.63, \"D4\": 293.66, \"E4\": 329.63, \"F4\": 349.23,\n",
    "    \"G4\": 392.00, \"A4\": 440.00, \"B4\": 493.88,\n",
    "    \"C5\": 523.25, \"D5\": 587.33, \"E5\": 659.25\n",
    "}\n",
    "SCALES = {\n",
    "    \"C_major\": [\"C4\",\"D4\",\"E4\",\"F4\",\"G4\",\"A4\",\"B4\",\"C5\",\"D5\",\"E5\"],\n",
    "    \"A_minor\": [\"A4\",\"B4\",\"C5\",\"D5\",\"E5\",\"G4\",\"A4\",\"C5\",\"D5\"],\n",
    "}\n",
    "def _sine(freq, t, sr):  # 기본 사인파\n",
    "    return np.sin(2*np.pi*freq*t)\n",
    "\n",
    "def synth_mock(brief: MusicBrief, seed: int = 42) -> Tuple[int, np.ndarray]:\n",
    "    sr = 44100\n",
    "    dur = int(brief.duration_sec)\n",
    "    t = np.linspace(0, dur, int(sr*dur), endpoint=False)\n",
    "\n",
    "    # 키에 따라 간단 스케일 선택\n",
    "    scale_name = \"C_major\"\n",
    "    if \"minor\" in brief.key.lower():\n",
    "        scale_name = \"A_minor\"\n",
    "    scale = [NOTE_FREQ.get(n, 440.0) for n in SCALES.get(scale_name, SCALES[\"C_major\"])]\n",
    "\n",
    "    bpm = int(brief.bpm)\n",
    "    beat = 60.0 / max(1, bpm)\n",
    "    note_len = beat * 0.95\n",
    "\n",
    "    audio = np.zeros_like(t)\n",
    "    rng = np.random.default_rng(seed)\n",
    "    pos = 0.0\n",
    "\n",
    "    while pos < dur:\n",
    "        f = float(rng.choice(scale))\n",
    "        on = pos\n",
    "        off = min(pos + note_len, dur)\n",
    "        seg = (t >= on) & (t < off)\n",
    "        tt = t[seg] - on\n",
    "\n",
    "        # 사인 + 약간의 하모닉\n",
    "        sig = 0.6*_sine(f, tt, sr) + 0.25*_sine(2*f, tt, sr) + 0.15*_sine(3*f, tt, sr)\n",
    "\n",
    "        # 간단 attack/release\n",
    "        attack = int(0.01*sr)\n",
    "        release = int(0.08*sr)\n",
    "        env = np.ones_like(sig)\n",
    "        env[:attack] = np.linspace(0, 1, attack, endpoint=False)\n",
    "        if release < len(env):\n",
    "            env[-release:] = np.linspace(1, 0, release, endpoint=False)\n",
    "\n",
    "        audio[seg] += sig * env\n",
    "        pos += beat\n",
    "\n",
    "    # 소프트 클리핑 + 노멀라이즈\n",
    "    audio = np.tanh(audio)\n",
    "    audio /= (np.max(np.abs(audio)) + 1e-8)\n",
    "    return sr, audio.astype(np.float32)\n",
    "\n",
    "# 7-3) (옵션) Replicate MusicGen 호출\n",
    "def generate_with_replicate(prompt: str, duration: int) -> Optional[str]:\n",
    "    token = os.getenv(\"REPLICATE_API_TOKEN\")\n",
    "    if not token:\n",
    "        return None\n",
    "    try:\n",
    "        import replicate  # 필요: pip install replicate\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    client = replicate.Client(api_token=token)\n",
    "    # 주의: 실제 환경에선 최신 식별자/버전 명시 권장\n",
    "    output = client.run(\n",
    "        \"meta/musicgen\",\n",
    "        input={\"prompt\": prompt, \"duration\": duration}\n",
    "    )\n",
    "    if isinstance(output, list) and output:\n",
    "        url = output[0]\n",
    "        # URL의 바이너리를 파일로 저장\n",
    "        os.makedirs(\"outputs\", exist_ok=True)\n",
    "        out = f\"outputs/musicgen_{int(time.time())}.wav\"\n",
    "        r = requests.get(url, timeout=60)\n",
    "        r.raise_for_status()\n",
    "        with open(out, \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "        return out\n",
    "    return None\n",
    "\n",
    "# 7-4) LangGraph 노드: brief -> 오디오 파일 생성\n",
    "def generate_music_node(state: GraphState) -> GraphState:\n",
    "    brief: MusicBrief = state[\"brief\"]\n",
    "    use_ext = state.get(\"use_external\", False)\n",
    "\n",
    "    os.makedirs(\"outputs\", exist_ok=True)\n",
    "    provider = \"mock\"\n",
    "    audio_path = None\n",
    "\n",
    "    # (1) 외부 모델 시도\n",
    "    if use_ext:\n",
    "        audio_path = generate_with_replicate(brief.prompt, int(brief.duration_sec))\n",
    "        if audio_path:\n",
    "            provider = \"replicate\"\n",
    "\n",
    "    # (2) 실패/미사용이면 Mock 합성\n",
    "    if audio_path is None:\n",
    "        sr, audio = synth_mock(brief)\n",
    "        audio_path = f\"outputs/mock_{int(time.time())}.wav\"\n",
    "        write_wav(audio_path, sr, audio)\n",
    "        provider = \"mock\"\n",
    "\n",
    "    # (3) 결과를 state에 적재 (다음 단계나 최종 출력에 사용)\n",
    "    state[\"audio_path\"] = audio_path\n",
    "    state[\"provider_used\"] = provider\n",
    "    state[\"meta\"] = {\n",
    "        \"emotion\": state[\"emotion\"].model_dump(),\n",
    "        \"brief\": state[\"brief\"].model_dump(),\n",
    "        \"provider\": provider,\n",
    "        \"path\": audio_path,\n",
    "    }\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06e17820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph_with_start():\n",
    "    builder = StateGraph(GraphState)\n",
    "\n",
    "    builder.add_node(\"analyze_emotion\", analyze_emotion_node)\n",
    "    builder.add_node(\"compose_brief\",  compose_brief_node)\n",
    "    builder.add_node(\"generate_music\", generate_music_node)\n",
    "\n",
    "    builder.add_edge(START, \"analyze_emotion\")\n",
    "    builder.add_edge(\"analyze_emotion\", \"compose_brief\")\n",
    "    builder.add_edge(\"compose_brief\",  \"generate_music\")\n",
    "    builder.add_edge(\"generate_music\", END)\n",
    "\n",
    "    app = builder.compile()\n",
    "    viz = app.get_graph()          # 🔁 여기! builder가 아니라 app에서 get_graph()\n",
    "    return app, viz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c42fc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Emotion ===\n",
      "{'primary': '긴장', 'valence': -0.5, 'arousal': 0.7, 'confidence': 0.8, 'reasons': '면접을 앞두고 느끼는 긴장감과 불안감 때문입니다.'}\n",
      "\n",
      "=== Music Brief ===\n",
      "{'mood': 'calming', 'bpm': 70, 'key': 'C major', 'duration_sec': 30, 'instruments': ['piano', 'strings'], 'style_tags': [], 'prompt': 'A soothing background music to help ease tension before an interview, featuring gentle piano and soft strings.'}\n",
      "\n",
      "=== Provider Used ===\n",
      "mock\n",
      "\n",
      "=== Audio Path ===\n",
      "outputs/mock_1757666887.wav\n"
     ]
    }
   ],
   "source": [
    "app, viz = build_graph_with_start()\n",
    "\n",
    "state = {\n",
    "    \"user_text\": \"오늘 면접을 앞두고 떨려요. 마음을 차분하게 다잡고 싶어요.\",\n",
    "    \"use_external\": False,   # True면 Replicate 사용(토큰 필요)\n",
    "}\n",
    "\n",
    "final = app.invoke(state)\n",
    "\n",
    "print(\"=== Emotion ===\")\n",
    "print(final[\"emotion\"].model_dump())\n",
    "\n",
    "print(\"\\n=== Music Brief ===\")\n",
    "print(final[\"brief\"].model_dump())\n",
    "\n",
    "print(\"\\n=== Provider Used ===\")\n",
    "print(final[\"provider_used\"])\n",
    "\n",
    "print(\"\\n=== Audio Path ===\")\n",
    "print(final[\"audio_path\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph-env-e8JFwEVd-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
