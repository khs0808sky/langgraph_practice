{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c158de7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60407c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "assert os.getenv(\"OPENAI_API_KEY\"), \"OPENAI_API_KEY ê°€ .envì— ì—†ì–´ìš”. (.envì— OPENAI_API_KEY=... ì¶”ê°€)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bfe84b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Literal, TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "Valence = Annotated[float, Field(ge=-1.0, le=1.0)]      # ì •ì„œì˜ ë°ê¸°(ë¶€ì • - ê¸ì •)\n",
    "Arousal = Annotated[float, Field(ge=0.0, le=1.0)]       # ê°ì„±ë„(ì—ë„ˆì§€)\n",
    "Confidence = Annotated[float, Field(ge=0.0, le=1.0)]    # í™•ì‹ ë„\n",
    "BPM     = Annotated[int,   Field(ge=50,  le=140)]       # ë¶„ë‹¹ ë¹„íŠ¸ìˆ˜(BPM)\n",
    "DurSec  = Annotated[int,   Field(ge=5,   le=60)]        # ìŒì•… ê¸¸ì´(5ì´ˆ - 60ì´ˆ) â€»ì„ì‹œ\n",
    "\n",
    "class EmotionResult(BaseModel):     # ê°ì • ë¶„ì„ ê²°ê³¼ì˜ í‹€\n",
    "    primary: str                    # í–‰ë³µ/ìŠ¬í””/ë¶ˆì•ˆ ê°™ì€ ì£¼ìš” ê°ì • ë¼ë²¨ (ë¬¸ìì—´)\n",
    "    valence: Valence = 0.0          # ë°ê¸°(-1~1) ê¸°ë³¸ê°’ 0\n",
    "    arousal: Arousal = 0.5          # ê°ì„±ë„(0~1) ê¸°ë³¸ê°’ 0.5\n",
    "    confidence: Confidence = 0.7    # í™•ì‹ ë„(0~1) ê¸°ë³¸ê°’ 0.7\n",
    "    reasons: str = \"â€”\"              # ê°ì • íŒë‹¨ì˜ ê·¼ê±° ìš”ì•½ ê¸°ë³¸ì€ â€”\n",
    "\n",
    "class MusicBrief(BaseModel):        # ìŒì•… ìƒì„± ì „ì— í•„ìš”í•œ ì„¤ê³„ì„œ(ë¸Œë¦¬í”„) ìŠ¤í‚¤ë§ˆ\n",
    "    mood: str                       # calm/uplifting/melancholic ê°™ì€ ì „ë°˜ ë¬´ë“œ\n",
    "    bpm: BPM = 90                   # 50~140 ë²”ìœ„ì˜ ì •ìˆ˜ ê¸°ë³¸ 90\n",
    "    key: str = \"C major\"            # ì¡°ì„±. ì˜ˆ: â€œC majorâ€, â€œA minorâ€\n",
    "    duration_sec: DurSec = 20       # 5~60 ë²”ìœ„ì˜ ì •ìˆ˜(ì´ˆ). ê¸°ë³¸ 20\n",
    "    instruments: List[str] = []     # í•µì‹¬ ì•…ê¸°ë“¤. ì˜ˆ: [\"piano\", \"strings\"].\n",
    "    style_tags: List[str] = []      # ìŠ¤íƒ€ì¼ íƒœê·¸. ì˜ˆ: [\"ambient\", \"cinematic\"].\n",
    "    prompt: str                     # í…ìŠ¤íŠ¸-íˆ¬-ë®¤ì§ ëª¨ë¸ì— ì¤„ ì˜ë¬¸ í”„ë¡¬í”„íŠ¸ (í•„ìˆ˜)\n",
    "\n",
    "class GraphState(TypedDict, total=False):\n",
    "    user_text: str                              # ì‚¬ìš©ì ì…ë ¥ ë¬¸ì¥\n",
    "    use_external: bool                          # ì™¸ë¶€ ëª¨ë¸(ì˜ˆ: Replicate) ì“¸ì§€ ì—¬ë¶€\n",
    "    emotion: EmotionResult                      # ìœ„ ìŠ¤í‚¤ë§ˆë¡œ ê²€ì¦ëœ ê°ì • ê²°ê³¼\n",
    "    brief: MusicBrief                           # ìœ„ ìŠ¤í‚¤ë§ˆë¡œ ê²€ì¦ëœ ìŒì•… ë¸Œë¦¬í”„\n",
    "    audio_path: str                             # ìƒì„±ëœ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ\n",
    "    provider_used: Literal[\"replicate\", \"mock\"] # ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ìƒì„±í–ˆë‚˜\n",
    "    meta: Dict                                  # ê·¸ ì™¸ ë¡œê·¸/ë””ë²„ê·¸ìš© ë©”íƒ€ë°ì´í„°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222c3e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm():\n",
    "    # OpenAI Chat ëª¨ë¸ì„ ë˜í•‘í•œ LangChain ê°ì²´\n",
    "    # temperature=0.2 â†’ ë” ì¼ê´€ëœ(ëœ ëœë¤) ì¶œë ¥\n",
    "    return ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
    "\n",
    "#ìŠ¤í‚¤ë§ˆ ê°•ì œ(Structured Output), ê°ì • ë¼ë²¨ë§: 0.0 ~ 0.3\n",
    "#ìŒì•… ë¸Œë¦¬í”„ ê°™ì´ ì‚´ì§ ì°½ì˜ì„± í•„ìš”: 0.3 ~ 0.6\n",
    "#ê°€ì‚¬/ìŠ¤í† ë¦¬ì²˜ëŸ¼ ì°½ì‘ì„± ê·¹ëŒ€í™”: 0.7 ~ 1.0 (í˜•ì‹ ê¹¨ì§ˆ ìˆ˜ ìˆìŒ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3f9579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_emotion_node(state: GraphState) -> GraphState: # ê°ì • ë¶„ì„ ë…¸ë“œ\n",
    "    llm = get_llm()\n",
    "\n",
    "    # ì‹œìŠ¤í…œ ì§€ì‹œë¬¸: ì—­í• /ì¶œë ¥ í˜•ì‹ì„ LLMì—ê²Œ ëª…í™•íˆ ì§€ì •\n",
    "    sys = (\n",
    "        \"ë‹¹ì‹ ì€ ì‹¬ë¦¬ ì •ì„œë¥¼ ìš”ì•½í•˜ëŠ” ë¶„ì„ê°€ì…ë‹ˆë‹¤. \"\n",
    "        \"ì‚¬ìš©ì í…ìŠ¤íŠ¸ë¡œë¶€í„° ì£¼ìš” ê°ì •ì„ í•œ ë‹¨ì–´(ë˜ëŠ” ì§§ì€ êµ¬)ë¡œ ë„ì¶œí•˜ê³ , \"\n",
    "        \"valence(-1~1)ì™€ arousal(0~1), confidence(0~1)ë¥¼ ì¶”ì •í•˜ì„¸ìš”. \"\n",
    "        \"ë°˜ë“œì‹œ ì£¼ì–´ì§„ JSON ìŠ¤í‚¤ë§ˆ(EmotionResult)ì— ë§ì¶° ì‘ë‹µí•˜ì„¸ìš”.\"\n",
    "    )\n",
    "\n",
    "    # ì‚¬ìš©ì ì…ë ¥ì€ stateì—ì„œ êº¼ëƒ„ (ê·¸ë˜í”„ì˜ 'ê°€ë°©'ì—ì„œ user_textë¥¼ ì½ì–´ì˜¤ëŠ” ëŠë‚Œ)\n",
    "    user_msg = state['user_text']\n",
    "\n",
    "    # LLM ì¶œë ¥ì´ EmotionResult ìŠ¤í‚¤ë§ˆì— 'ìë™ìœ¼ë¡œ ê²€ì¦/íŒŒì‹±'ë˜ë„ë¡ ë˜í•‘\n",
    "    structured = llm.with_structured_output(EmotionResult)\n",
    "\n",
    "    # ë©”ì‹œì§€ í˜•ì‹: system + user\n",
    "    result = structured.invoke([\n",
    "        {\"role\": \"system\", \"content\": sys},\n",
    "        {\"role\": \"user\",   \"content\": user_msg}\n",
    "    ])\n",
    "\n",
    "    # ê²°ê³¼ë¥¼ stateì— ì €ì¥í•˜ê³  ë‹¤ìŒ ë…¸ë“œê°€ ì“°ê²Œ í•œë‹¤\n",
    "    state['emotion'] = result\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80e66319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose_brief_node(state: GraphState) -> GraphState:\n",
    "    llm = get_llm()                            # 1) ê°™ì€ LLM ì„¤ì • ì¬ì‚¬ìš©\n",
    "    emo: EmotionResult = state[\"emotion\"]      # 2) ë°©ê¸ˆ ì €ì¥í•œ ê°ì • ê²°ê³¼ êº¼ë‚´ê¸°\n",
    "\n",
    "    # 3) ì‹œìŠ¤í…œ ì§€ì‹œë¬¸: ì¶œë ¥ í˜•ì‹(ë¸Œë¦¬í”„ ê·œì¹™)ê³¼ ì œì•½(bpm, duration, key)ì„ ì„¤ëª…\n",
    "    sys = (\n",
    "        \"ë„ˆëŠ” ìŒì•… ê°ë…ì´ë‹¤. ì•„ë˜ ê°ì • ë¶„ì„ ê²°ê³¼ì™€ ì‚¬ìš©ì í…ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•´, \"\n",
    "        \"ì¹˜ìœ /ì•ˆì • ëª©ì ì˜ ì§§ì€ BGMì— ì í•©í•œ Music Briefë¥¼ JSONìœ¼ë¡œ ë§Œë“¤ì–´ë¼. \"\n",
    "        \"bpmì€ 50~140, duration_secëŠ” 5~60 ì‚¬ì´ì—¬ì•¼ í•˜ë©°, keyëŠ” 'C major' ê°™ì€ í˜•ì‹. \"\n",
    "        \"promptëŠ” ì˜ì–´ë¡œ, í•µì‹¬ ì•…ê¸°/ë¬´ë“œ/ë‹¤ì´ë‚´ë¯¹ì„ ê°„ê²°íˆ í¬í•¨í•  ê²ƒ.\"\n",
    "    )\n",
    "\n",
    "    # 4) ì‚¬ìš©ì ë©”ì‹œì§€: ê°ì • ìˆ˜ì¹˜ì™€ ì›ë¬¸ í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ì „ë‹¬\n",
    "    usr = (\n",
    "        f\"# Emotion\\n\"\n",
    "        f\"primary={emo.primary}, valence={emo.valence}, arousal={emo.arousal}, confidence={emo.confidence}\\n\\n\"\n",
    "        f\"# Text\\n{state['user_text']}\\n\"\n",
    "    )\n",
    "\n",
    "    # 5) MusicBrief ìŠ¤í‚¤ë§ˆë¡œ êµ¬ì¡°í™” ì¶œë ¥ ê°•ì œ\n",
    "    structured = llm.with_structured_output(MusicBrief)\n",
    "    brief = structured.invoke([\n",
    "        {\"role\": \"system\", \"content\": sys},\n",
    "        {\"role\": \"user\",   \"content\": usr}\n",
    "    ])\n",
    "\n",
    "    state[\"brief\"] = brief                    # 6) ë‹¤ìŒ ë…¸ë“œì—ì„œ ì“°ë„ë¡ ê°€ë°©ì— ë‹´ê¸°\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bc2e88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 7: ìŒì•… ìƒì„± ë…¸ë“œ (Replicate or Mock) ---\n",
    "import time\n",
    "import wave\n",
    "import struct\n",
    "import requests\n",
    "import numpy as np\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# 7-1) ê°„ë‹¨ WAV ì €ì¥ê¸° (16-bit PCM, ëª¨ë…¸)\n",
    "def write_wav(path: str, sr: int, audio: np.ndarray):\n",
    "    data = np.clip(audio, -1.0, 1.0)\n",
    "    data = (data * 32767.0).astype(np.int16)  # float[-1,1] -> int16\n",
    "    with wave.open(path, 'wb') as wf:\n",
    "        wf.setnchannels(1)      # mono\n",
    "        wf.setsampwidth(2)      # 16-bit\n",
    "        wf.setframerate(sr)\n",
    "        wf.writeframes(data.tobytes())\n",
    "\n",
    "# 7-2) Mock í•©ì„±ê¸°: ê°„ë‹¨í•œ ì‚¬ì¸íŒŒ ë©œë¡œë””(ìŠ¤ì¼€ì¼ + BPM ê¸°ë°˜)\n",
    "NOTE_FREQ = {\n",
    "    \"C4\": 261.63, \"D4\": 293.66, \"E4\": 329.63, \"F4\": 349.23,\n",
    "    \"G4\": 392.00, \"A4\": 440.00, \"B4\": 493.88,\n",
    "    \"C5\": 523.25, \"D5\": 587.33, \"E5\": 659.25\n",
    "}\n",
    "SCALES = {\n",
    "    \"C_major\": [\"C4\",\"D4\",\"E4\",\"F4\",\"G4\",\"A4\",\"B4\",\"C5\",\"D5\",\"E5\"],\n",
    "    \"A_minor\": [\"A4\",\"B4\",\"C5\",\"D5\",\"E5\",\"G4\",\"A4\",\"C5\",\"D5\"],\n",
    "}\n",
    "def _sine(freq, t, sr):  # ê¸°ë³¸ ì‚¬ì¸íŒŒ\n",
    "    return np.sin(2*np.pi*freq*t)\n",
    "\n",
    "def synth_mock(brief: MusicBrief, seed: int = 42) -> Tuple[int, np.ndarray]:\n",
    "    sr = 44100\n",
    "    dur = int(brief.duration_sec)\n",
    "    t = np.linspace(0, dur, int(sr*dur), endpoint=False)\n",
    "\n",
    "    # í‚¤ì— ë”°ë¼ ê°„ë‹¨ ìŠ¤ì¼€ì¼ ì„ íƒ\n",
    "    scale_name = \"C_major\"\n",
    "    if \"minor\" in brief.key.lower():\n",
    "        scale_name = \"A_minor\"\n",
    "    scale = [NOTE_FREQ.get(n, 440.0) for n in SCALES.get(scale_name, SCALES[\"C_major\"])]\n",
    "\n",
    "    bpm = int(brief.bpm)\n",
    "    beat = 60.0 / max(1, bpm)\n",
    "    note_len = beat * 0.95\n",
    "\n",
    "    audio = np.zeros_like(t)\n",
    "    rng = np.random.default_rng(seed)\n",
    "    pos = 0.0\n",
    "\n",
    "    while pos < dur:\n",
    "        f = float(rng.choice(scale))\n",
    "        on = pos\n",
    "        off = min(pos + note_len, dur)\n",
    "        seg = (t >= on) & (t < off)\n",
    "        tt = t[seg] - on\n",
    "\n",
    "        # ì‚¬ì¸ + ì•½ê°„ì˜ í•˜ëª¨ë‹‰\n",
    "        sig = 0.6*_sine(f, tt, sr) + 0.25*_sine(2*f, tt, sr) + 0.15*_sine(3*f, tt, sr)\n",
    "\n",
    "        # ê°„ë‹¨ attack/release\n",
    "        attack = int(0.01*sr)\n",
    "        release = int(0.08*sr)\n",
    "        env = np.ones_like(sig)\n",
    "        env[:attack] = np.linspace(0, 1, attack, endpoint=False)\n",
    "        if release < len(env):\n",
    "            env[-release:] = np.linspace(1, 0, release, endpoint=False)\n",
    "\n",
    "        audio[seg] += sig * env\n",
    "        pos += beat\n",
    "\n",
    "    # ì†Œí”„íŠ¸ í´ë¦¬í•‘ + ë…¸ë©€ë¼ì´ì¦ˆ\n",
    "    audio = np.tanh(audio)\n",
    "    audio /= (np.max(np.abs(audio)) + 1e-8)\n",
    "    return sr, audio.astype(np.float32)\n",
    "\n",
    "# 7-3) (ì˜µì…˜) Replicate MusicGen í˜¸ì¶œ\n",
    "def generate_with_replicate(prompt: str, duration: int) -> Optional[str]:\n",
    "    token = os.getenv(\"REPLICATE_API_TOKEN\")\n",
    "    if not token:\n",
    "        return None\n",
    "    try:\n",
    "        import replicate  # í•„ìš”: pip install replicate\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    client = replicate.Client(api_token=token)\n",
    "    # ì£¼ì˜: ì‹¤ì œ í™˜ê²½ì—ì„  ìµœì‹  ì‹ë³„ì/ë²„ì „ ëª…ì‹œ ê¶Œì¥\n",
    "    output = client.run(\n",
    "        \"meta/musicgen\",\n",
    "        input={\"prompt\": prompt, \"duration\": duration}\n",
    "    )\n",
    "    if isinstance(output, list) and output:\n",
    "        url = output[0]\n",
    "        # URLì˜ ë°”ì´ë„ˆë¦¬ë¥¼ íŒŒì¼ë¡œ ì €ì¥\n",
    "        os.makedirs(\"outputs\", exist_ok=True)\n",
    "        out = f\"outputs/musicgen_{int(time.time())}.wav\"\n",
    "        r = requests.get(url, timeout=60)\n",
    "        r.raise_for_status()\n",
    "        with open(out, \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "        return out\n",
    "    return None\n",
    "\n",
    "# 7-4) LangGraph ë…¸ë“œ: brief -> ì˜¤ë””ì˜¤ íŒŒì¼ ìƒì„±\n",
    "def generate_music_node(state: GraphState) -> GraphState:\n",
    "    brief: MusicBrief = state[\"brief\"]\n",
    "    use_ext = state.get(\"use_external\", False)\n",
    "\n",
    "    os.makedirs(\"outputs\", exist_ok=True)\n",
    "    provider = \"mock\"\n",
    "    audio_path = None\n",
    "\n",
    "    # (1) ì™¸ë¶€ ëª¨ë¸ ì‹œë„\n",
    "    if use_ext:\n",
    "        audio_path = generate_with_replicate(brief.prompt, int(brief.duration_sec))\n",
    "        if audio_path:\n",
    "            provider = \"replicate\"\n",
    "\n",
    "    # (2) ì‹¤íŒ¨/ë¯¸ì‚¬ìš©ì´ë©´ Mock í•©ì„±\n",
    "    if audio_path is None:\n",
    "        sr, audio = synth_mock(brief)\n",
    "        audio_path = f\"outputs/mock_{int(time.time())}.wav\"\n",
    "        write_wav(audio_path, sr, audio)\n",
    "        provider = \"mock\"\n",
    "\n",
    "    # (3) ê²°ê³¼ë¥¼ stateì— ì ì¬ (ë‹¤ìŒ ë‹¨ê³„ë‚˜ ìµœì¢… ì¶œë ¥ì— ì‚¬ìš©)\n",
    "    state[\"audio_path\"] = audio_path\n",
    "    state[\"provider_used\"] = provider\n",
    "    state[\"meta\"] = {\n",
    "        \"emotion\": state[\"emotion\"].model_dump(),\n",
    "        \"brief\": state[\"brief\"].model_dump(),\n",
    "        \"provider\": provider,\n",
    "        \"path\": audio_path,\n",
    "    }\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06e17820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph_with_start():\n",
    "    builder = StateGraph(GraphState)\n",
    "\n",
    "    builder.add_node(\"analyze_emotion\", analyze_emotion_node)\n",
    "    builder.add_node(\"compose_brief\",  compose_brief_node)\n",
    "    builder.add_node(\"generate_music\", generate_music_node)\n",
    "\n",
    "    builder.add_edge(START, \"analyze_emotion\")\n",
    "    builder.add_edge(\"analyze_emotion\", \"compose_brief\")\n",
    "    builder.add_edge(\"compose_brief\",  \"generate_music\")\n",
    "    builder.add_edge(\"generate_music\", END)\n",
    "\n",
    "    app = builder.compile()\n",
    "    viz = app.get_graph()          # ğŸ” ì—¬ê¸°! builderê°€ ì•„ë‹ˆë¼ appì—ì„œ get_graph()\n",
    "    return app, viz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c42fc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Emotion ===\n",
      "{'primary': 'ê¸´ì¥', 'valence': -0.5, 'arousal': 0.7, 'confidence': 0.8, 'reasons': 'ë©´ì ‘ì„ ì•ë‘ê³  ëŠë¼ëŠ” ê¸´ì¥ê°ê³¼ ë¶ˆì•ˆê° ë•Œë¬¸ì…ë‹ˆë‹¤.'}\n",
      "\n",
      "=== Music Brief ===\n",
      "{'mood': 'calming', 'bpm': 70, 'key': 'C major', 'duration_sec': 30, 'instruments': ['piano', 'strings'], 'style_tags': [], 'prompt': 'A soothing background music to help ease tension before an interview, featuring gentle piano and soft strings.'}\n",
      "\n",
      "=== Provider Used ===\n",
      "mock\n",
      "\n",
      "=== Audio Path ===\n",
      "outputs/mock_1757666887.wav\n"
     ]
    }
   ],
   "source": [
    "app, viz = build_graph_with_start()\n",
    "\n",
    "state = {\n",
    "    \"user_text\": \"ì˜¤ëŠ˜ ë©´ì ‘ì„ ì•ë‘ê³  ë–¨ë ¤ìš”. ë§ˆìŒì„ ì°¨ë¶„í•˜ê²Œ ë‹¤ì¡ê³  ì‹¶ì–´ìš”.\",\n",
    "    \"use_external\": False,   # Trueë©´ Replicate ì‚¬ìš©(í† í° í•„ìš”)\n",
    "}\n",
    "\n",
    "final = app.invoke(state)\n",
    "\n",
    "print(\"=== Emotion ===\")\n",
    "print(final[\"emotion\"].model_dump())\n",
    "\n",
    "print(\"\\n=== Music Brief ===\")\n",
    "print(final[\"brief\"].model_dump())\n",
    "\n",
    "print(\"\\n=== Provider Used ===\")\n",
    "print(final[\"provider_used\"])\n",
    "\n",
    "print(\"\\n=== Audio Path ===\")\n",
    "print(final[\"audio_path\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph-env-e8JFwEVd-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
