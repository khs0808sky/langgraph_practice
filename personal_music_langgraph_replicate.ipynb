{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc6a60bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0d072e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI ok: True\n",
      "REPLICATE ok: True\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(usecwd=True), override=True)\n",
    "\n",
    "import os\n",
    "print(\"OPENAI ok:\", bool(os.getenv(\"OPENAI_API_KEY\")))\n",
    "print(\"REPLICATE ok:\", bool(os.getenv(\"REPLICATE_API_TOKEN\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e2ed431b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 임포트\n",
    "from typing import List, Dict, Literal, TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, START, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "debf9c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 스키마 정의 (EmotionResult / MusicBrief / GraphState)\n",
    "Valence = Annotated[float, Field(ge=-1.0, le=1.0)]\n",
    "Arousal = Annotated[float, Field(ge=0.0, le=1.0)]\n",
    "BPM     = Annotated[int,   Field(ge=50,  le=140)]\n",
    "DurSec  = Annotated[int,   Field(ge=60,   le=90)] # 음악 재생시간(60초 ~ 90초)\n",
    "\n",
    "class EmotionResult(BaseModel):\n",
    "    primary: str\n",
    "    valence: Valence = 0.0\n",
    "    arousal: Arousal = 0.5\n",
    "    confidence: Arousal = 0.7\n",
    "    reasons: str = \"—\"\n",
    "\n",
    "class MusicBrief(BaseModel):\n",
    "    mood: str\n",
    "    bpm: BPM = 90\n",
    "    key: str\n",
    "    duration_sec: DurSec = 60               # 음악 기본 재생시간 60초\n",
    "    instruments: List[str] = []\n",
    "    style_tags: List[str] = []\n",
    "    prompt: str  # 영어 프롬프트\n",
    "\n",
    "class GraphState(TypedDict, total=False):\n",
    "    user_text: str\n",
    "    emotion: EmotionResult\n",
    "    brief: MusicBrief\n",
    "    audio_path: str\n",
    "    provider_used: Literal[\"replicate\"]\n",
    "    meta: Dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6f4057eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) LLM 헬퍼 (OpenAI)\n",
    "def get_llm():\n",
    "    # 스키마 맞춤 출력이 중요 → 낮은 temperature\n",
    "    return ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "36626a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) 감정 분석 노드\n",
    "def analyze_emotion_node(state: GraphState) -> GraphState:\n",
    "    llm = get_llm()\n",
    "    sys = (\n",
    "        \"당신은 심리 정서를 요약하는 분석가입니다. \"\n",
    "        \"사용자 텍스트에서 주요 감정을 한 단어(또는 짧은 구)로 도출하고, \"\n",
    "        \"valence(-1~1), arousal(0~1), confidence(0~1)을 추정하세요. \"\n",
    "        \"반드시 EmotionResult(JSON 스키마)에 맞춰 응답하세요.\"\n",
    "    )\n",
    "    structured = llm.with_structured_output(EmotionResult)\n",
    "    result = structured.invoke([\n",
    "        {\"role\":\"system\",\"content\":sys},\n",
    "        {\"role\":\"user\",\"content\":state[\"user_text\"]}\n",
    "    ])\n",
    "    state[\"emotion\"] = result\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d8c3ad5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) 음악 브리프 노드\n",
    "def compose_brief_node(state: GraphState) -> GraphState:\n",
    "    llm = get_llm()\n",
    "    emo: EmotionResult = state[\"emotion\"]\n",
    "    sys = (\n",
    "    \"너는 음악 감독이다. 아래 감정 분석과 사용자 텍스트를 참고해 \"\n",
    "    \"치유/안정 목적의 짧은 BGM을 위한 Music Brief를 JSON으로 만들어라.\\n\"\n",
    "    \"대체로 다음 규칙을 따른다:\\n\"\n",
    "    \"1) bpm: 50~140. arousal이 높을수록 빠르게(대략 60 + arousal*40), 낮을수록 느리게.\\n\"\n",
    "    \"2) duration_sec: 60~90. valence<=-0.2 또는 arousal>0.6이면 78~90, 그 외 60~78.\\n\"\n",
    "    \"3) key: valence>=0.2 → 메이저(C/G/F/D major 등), valence<=-0.2 → 마이너(A/D/E/B minor 등), 그 외엔 두 계열 중 선택. \"\n",
    "    \"'C major'를 기본값으로 반복 사용하지 말 것.\\n\"\n",
    "    \"4) instruments: 2~4개. 기본은 warm piano, soft pad 중심. \"\n",
    "    \"불안/고각성(arousal>0.6)에는 light percussion/gentle pulse를 소량, \"\n",
    "    \"우울(valence<-0.2)에는 strings 또는 soft choir를 권장.\\n\"\n",
    "    \"5) style_tags: 3~5개 (예: calming, minimal, warm, ambient, breathing).\\n\"\n",
    "    \"6) prompt: 영어 한 문장, 25단어 이내. 악기/무드/다이내믹/리듬 강도(soft/very soft 등)를 포함하되 \"\n",
    "    \"숫자(bpm/duration/key)는 넣지 말 것.\\n\"\n",
    "    \"7) JSON만 출력. 추가 설명 금지.\\n\"\n",
    "    )  \n",
    "\n",
    "    usr = (\n",
    "        f\"# Emotion\\nprimary={emo.primary}, valence={emo.valence}, \"\n",
    "        f\"arousal={emo.arousal}, confidence={emo.confidence}\\n\\n\"\n",
    "        f\"# Text\\n{state['user_text']}\\n\"\n",
    "    )\n",
    "    structured = llm.with_structured_output(MusicBrief)\n",
    "    brief = structured.invoke([\n",
    "        {\"role\":\"system\",\"content\":sys},\n",
    "        {\"role\":\"user\",\"content\":usr}\n",
    "    ])\n",
    "\n",
    "# ▼ duration 보정 (60~90초로 강제)\n",
    "    if brief.duration_sec < 60:\n",
    "        brief = brief.model_copy(update={\"duration_sec\": 60})\n",
    "    elif brief.duration_sec > 90:\n",
    "        brief = brief.model_copy(update={\"duration_sec\": 90})\n",
    "\n",
    "    state[\"brief\"] = brief\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fd8cb784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 5) Replicate(Stable Audio 2.5) 호출 함수\n",
    "# 상단 import에 추가\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def generate_with_replicate_strict(prompt: str, duration: int) -> str:\n",
    "    tok = os.getenv(\"REPLICATE_API_TOKEN\")\n",
    "    assert tok, \"REPLICATE_API_TOKEN이 없습니다 (.env 확인)\"\n",
    "    assert 60 <= int(duration) <= 90, f\"duration(초)은 60~90 범위여야 합니다: {duration}\"\n",
    "\n",
    "    # 최신 클라이언트는 기본적으로 FileOutput을 반환\n",
    "    out = replicate.run(MODEL_ID, input={\"prompt\": prompt, \"duration\": int(duration)})\n",
    "    first = out[0] if isinstance(out, (list, tuple)) else out\n",
    "\n",
    "    os.makedirs(\"outputs\", exist_ok=True)\n",
    "    ts = int(time.time())\n",
    "\n",
    "    # 1) FileOutput (권장 경로) - .read()로 바로 저장\n",
    "    if hasattr(first, \"read\"):  # file-like (FileOutput)\n",
    "        # 확장자 추정: URL 있으면 거기서 뽑기\n",
    "        ext = \".bin\"\n",
    "        url_attr = getattr(first, \"url\", None)  # FileOutput은 .url을 제공\n",
    "        if isinstance(url_attr, str):\n",
    "            ext_candidate = os.path.splitext(urlparse(url_attr).path)[1].lower()\n",
    "            if ext_candidate:\n",
    "                ext = ext_candidate\n",
    "        out_path = f\"outputs/stableaudio_{ts}{ext}\"\n",
    "        with open(out_path, \"wb\") as f:\n",
    "            f.write(first.read())  # 전체 바이트 저장\n",
    "        return out_path\n",
    "\n",
    "    # 2) 과거/옵트아웃 경로: URL 문자열\n",
    "    if isinstance(first, str):\n",
    "        r = requests.get(first, timeout=120); r.raise_for_status()\n",
    "        ct = (r.headers.get(\"Content-Type\") or \"\").lower()\n",
    "        # content-type 또는 URL에서 확장자 추정\n",
    "        if \"wav\" in ct:\n",
    "            ext = \".wav\"\n",
    "        elif \"mpeg\" in ct or \"mp3\" in ct:\n",
    "            ext = \".mp3\"\n",
    "        else:\n",
    "            ext = os.path.splitext(urlparse(first).path)[1] or \".bin\"\n",
    "        out_path = f\"outputs/stableaudio_{ts}{ext}\"\n",
    "        with open(out_path, \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "        return out_path\n",
    "\n",
    "    # 3) 드물게 dict 형태로 오는 경우\n",
    "    if isinstance(first, dict):\n",
    "        url = first.get(\"url\") or first.get(\"audio\") or first.get(\"output\")\n",
    "        if isinstance(url, str):\n",
    "            r = requests.get(url, timeout=120); r.raise_for_status()\n",
    "            ext = os.path.splitext(urlparse(url).path)[1] or \".bin\"\n",
    "            out_path = f\"outputs/stableaudio_{ts}{ext}\"\n",
    "            with open(out_path, \"wb\") as f:\n",
    "                f.write(r.content)\n",
    "            return out_path\n",
    "\n",
    "    raise RuntimeError(f\"Unexpected replicate output type: {type(first)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a8eebb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) 음악 생성 노드 (Replicate만)\n",
    "def generate_music_node(state: GraphState) -> GraphState:\n",
    "    brief: MusicBrief = state[\"brief\"]\n",
    "    path = generate_with_replicate_strict(brief.prompt, int(brief.duration_sec))\n",
    "\n",
    "    state[\"audio_path\"] = path\n",
    "    state[\"provider_used\"] = \"replicate\"\n",
    "    state[\"meta\"] = {\n",
    "        \"emotion\": state[\"emotion\"].model_dump(),\n",
    "        \"brief\": state[\"brief\"].model_dump(),\n",
    "        \"provider\": \"replicate\",\n",
    "        \"path\": path,\n",
    "    }\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "344a40cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(GraphState)\n",
    "workflow.add_node(\"analyze_emotion\", analyze_emotion_node)\n",
    "workflow.add_node(\"compose_brief\",  compose_brief_node)\n",
    "workflow.add_node(\"generate_music\", generate_music_node)\n",
    "\n",
    "workflow.add_edge(START, \"analyze_emotion\")\n",
    "workflow.add_edge(\"analyze_emotion\", \"compose_brief\")\n",
    "workflow.add_edge(\"compose_brief\",  \"generate_music\")\n",
    "workflow.add_edge(\"generate_music\", END)\n",
    "\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4832f17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Emotion ===\n",
      "{'primary': '기쁨', 'valence': 1.0, 'arousal': 0.8, 'confidence': 0.9, 'reasons': '시험 합격이라는 긍정적인 경험으로 인해 기쁜 감정이 표현되고 있으며, 들뜬 상태와 발걸음이 빨라지는 것은 높은 각성을 나타냅니다.'}\n",
      "\n",
      "=== Music Brief ===\n",
      "{'mood': 'Joyful', 'bpm': 100, 'key': 'G major', 'duration_sec': 70, 'instruments': ['warm piano', 'soft pad', 'light percussion'], 'style_tags': ['uplifting', 'energetic', 'bright', 'minimal'], 'prompt': 'A bright and uplifting piece featuring warm piano and soft pads, with a gentle pulse to enhance the joyful mood.'}\n",
      "\n",
      "=== Provider Used ===\n",
      "replicate\n",
      "\n",
      "=== Audio Path ===\n",
      "outputs/stableaudio_1758012347.mp3\n"
     ]
    }
   ],
   "source": [
    "state = {\n",
    "    \"user_text\": \"시험에 합격해서 하루 종일 들떠. 발걸음이 저절로 빨라져\"\n",
    "}\n",
    "final = graph.invoke(state)\n",
    "\n",
    "print(\"=== Emotion ===\")\n",
    "print(final[\"emotion\"].model_dump())\n",
    "print(\"\\n=== Music Brief ===\")\n",
    "print(final[\"brief\"].model_dump())\n",
    "print(\"\\n=== Provider Used ===\")\n",
    "print(final[\"provider_used\"])\n",
    "print(\"\\n=== Audio Path ===\")\n",
    "print(final[\"audio_path\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph-env-e8JFwEVd-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
